import pandas as pd
import numpy as np

data = pd.read_csv("Supervised ML model\dataset2.csv")
print(data.head())

data = data[data.columns[5:]]
print(data.head())

#     // Define input test data
#     int expected_result = 1;
#     float input[INPUT_SIZE] = {-0.0737, -0.5675, -0.7121, -0.1424, 1.2619, -1.0615};

# Define the weights and biases
float w1[INPUT_SIZE][LAYER_1_SIZE] = {{-2.168393850326538086e+00, 8.165267109870910645e-01, -2.462978661060333252e-01, 8.624218106269836426e-01, 3.907405436038970947e-01, 1.153345033526420593e-01, -6.988520622253417969e-01, -1.517462491989135742e+00, -4.089244008064270020e-01, -2.631843686103820801e-01, 1.021683692932128906e+00, 1.348293304443359375e+00, -2.168457031250000000e+00, 6.911942958831787109e-01, -6.884197592735290527e-01, -1.740423440933227539e-01},
                                      {3.614163398742675781e-01, -3.088547289371490479e-01, -1.572295427322387695e+00, 4.252234939485788345e-03, 2.816461920738220215e-01, 6.871625781059265137e-02, -6.955011487007141113e-01, -7.091073393821716309e-01, -
                                          2.523320615291595459e-01, -1.368805527687072754e+00, -1.159423947334289551e+00, 9.221286773681640625e-01, 1.184956263750791550e-02, -2.051477432250976562e-01, 1.968548446893692017e-01, 4.029632806777954102e-01},
                                      {-1.111294150352478027e+00, -5.089741945266723633e-01, 1.073579668998718262e+00, -1.566225290298461914e+00, 6.429200768470764160e-01, 2.364763617515563965e-01, -6.188266873359680176e-01, -2.489659003913402557e-02, -
                                          5.672585368156433105e-01, 7.265736460685729980e-01, 2.489911615848541260e-01, 1.752165794372558594e+00, 9.181356430053710938e-02, 1.503776758909225464e-01, -6.006140708923339844e-01, -1.721028447151184082e+00},
                                      {3.078997731208801270e-01, 1.456468850374221802e-01, 8.725393414497375488e-01, -7.684517502784729004e-01, 1.093004941940307617e+00, 8.406445384025573730e-01, -1.788665652275085449e-01, -1.255702972412109375e-02, -
                                          7.559160590171813965e-01, 4.240052029490470886e-02, 2.292045056819915771e-01, 1.935747265815734863e-01, -6.396816372871398926e-01, -9.089657068252563477e-01, -3.948103487491607666e-01, 3.445330262184143066e-01},
                                      {1.499862223863601685e-01, 4.042107760906219482e-01, 1.635700762271881104e-01, 1.366754770278930664e-01, 4.144740477204322815e-02, 9.427173435688018799e-02, 1.522679924964904785e-01, -4.956104606389999390e-02, -
                                          5.468121767044067383e-01, 3.587086796760559082e-01, 1.542458310723304749e-02, 1.477928161621093750e-01, -1.823126375675201416e-01, 4.919950664043426514e-02, 2.097921222448348999e-01, -5.473721772432327271e-02},
                                      {4.940494596958160400e-01, 1.546553820371627808e-01, 6.714397668838500977e-01, 2.393311262130737305e-01, 5.030915737152099609e-01, -2.382779359817504883e+00, -1.173959612846374512e+00, 4.478949606418609619e-01, -4.133813083171844482e-01, -9.091829657554626465e-01, 1.649590432643890381e-01, -1.060984134674072266e-01, 4.669719040393829346e-01, -1.270243644714355469e+00, 1.212840676307678223e+00, -5.785285234451293945e-01}}
float b1[LAYER_1_SIZE] = {5.801272988319396973e-01,
                          1.217306613922119141e+00,
                          -1.746443472802639008e-02,
                          -3.477190434932708740e-01,
                          3.305948525667190552e-02,
                          2.259173989295959473e-01,
                          5.383665561676025391e-01,
                          9.012836813926696777e-01,
                          2.142165154218673706e-01,
                          3.288935124874114990e-01,
                          1.050016880035400391e+00,
                          -7.668792456388473511e-02,
                          6.212225556373596191e-01,
                          4.205402731895446777e-02,
                          -1.017153114080429077e-01,
                          9.458225965499877930e-01}
float w2[LAYER_1_SIZE][LAYER_2_SIZE] = {{-4.597735404968261719e-01, -4.789455235004425049e-01, -4.849088191986083984e-01, 1.507316112518310547e+00, -1.085607409477233887e+00, 4.840080738067626953e-01, -1.206181526184082031e+00, -6.284255981445312500e-01},
                                        {-6.987492442131042480e-01, 1.540628910064697266e+00, 7.484700083732604980e-01, -3.019030690193176270e-01, -
                                            3.906167149543762207e-01, 2.030662633478641510e-02, 3.528239727020263672e-01, 8.654546737670898438e-01},
                                        {1.031203866004943848e+00, -1.165139317512512207e+00, -7.906323671340942383e-01, -4.736455678939819336e-01,
                                            7.909241914749145508e-01, -2.202692776918411255e-01, 7.288138270378112793e-01, -3.416520655155181885e-01},
                                        {5.787907540798187256e-02, -3.339634537696838379e-01, -4.735736250877380371e-01, -4.847299754619598389e-01,
                                            2.404672205448150635e-01, 6.741912662982940674e-02, -1.134887456893920898e+00, 4.726754426956176758e-01},
                                        {6.570042967796325684e-01, 1.088284850120544434e+00, 6.209959983825683594e-01, 5.765162110328674316e-01, -
                                            5.329477041959762573e-02, 7.247991561889648438e-01, -2.052123695611953735e-01, -1.391128152608871460e-01},
                                        {-2.593374848365783691e-01, 1.745108217000961304e-01, -6.446945667266845703e-01, 3.965688049793243408e-01,
                                            4.817143976688385010e-01, 5.156488418579101562e-01, -6.817417144775390625e-01, 1.841758251190185547e+00},
                                        {-5.362457633018493652e-01, 1.309167981147766113e+00, -6.585267186164855957e-01, -8.630581200122833252e-02,
                                            9.543355107307434082e-01, -6.804478168487548828e-01, 2.871391475200653076e-01, 1.931405216455459595e-01},
                                        {1.584327816963195801e-01, 1.023035719990730286e-01, -4.131918251514434814e-01, -1.174916267395019531e+00, -
                                            3.628455698490142822e-01, -5.929613113403320312e-01, 1.572835922241210938e+00, 9.034239053726196289e-01},
                                        {6.126199960708618164e-01, -6.648458242416381836e-01, -1.458241343498229980e-01, 4.728476405143737793e-01, -
                                            5.320600867271423340e-01, -9.980551004409790039e-01, 6.910327672958374023e-01, -2.258397825062274933e-02},
                                        {9.468545317649841309e-01, -1.347116589546203613e+00, 6.136888265609741211e-01, 2.781690657138824463e-01, -
                                            5.904927253723144531e-01, -9.630287289619445801e-01, 3.354354202747344971e-01, -3.173164725303649902e-01},
                                        {-1.368297934532165527e+00, 8.239614963531494141e-01, 6.235236525535583496e-01, -2.476753294467926025e-01,
                                            6.119753718376159668e-01, 4.876955747604370117e-01, 3.879011869430541992e-01, -2.742928862571716309e-01},
                                        {1.283703804016113281e+00, -7.994807362556457520e-01, 3.966559767723083496e-01, 3.741462826728820801e-01,
                                            1.067042469978332520e+00, 1.150414347648620605e+00, -3.081003129482269287e-01, -8.505267500877380371e-01},
                                        {6.097160577774047852e-01, -8.488777875900268555e-01, -8.495675325393676758e-01, 8.132345974445343018e-02, -
                                            6.810644865036010742e-01, 1.427448511123657227e+00, -2.967551052570343018e-01, 1.301366329193115234e+00},
                                        {4.108251258730888367e-02, 5.047279596328735352e-01, 5.274659395217895508e-01, 6.266114115715026855e-01,
                                            6.636956334114074707e-01, 4.266961514949798584e-01, -6.170772314071655273e-01, -1.574778199195861816e+00},
                                        {-6.547189354896545410e-01, -6.772140860557556152e-01, -1.060588210821151733e-01, 4.711792171001434326e-01,
                                            1.781759619712829590e+00, 1.052639245986938477e+00, -5.973589420318603516e-01, 6.014434993267059326e-02},
                                        {5.452188849449157715e-01, -1.587060093879699707e+00, -6.113871335983276367e-01, -1.056253194808959961e+00, 3.401980176568031311e-02, -7.179572433233261108e-02, -5.789873003959655762e-01, -1.164129018783569336e+00}}
float b2[LAYER_2_SIZE] = {-1.980807483196258545e-01,
                          5.376006364822387695e-01,
                          8.836091756820678711e-01,
                          3.331608176231384277e-01,
                          -1.430760025978088379e-01,
                          1.786050386726856232e-02,
                          6.750583052635192871e-01,
                          2.842857539653778076e-01}
float w3[LAYER_2_SIZE][OUTPUT_SIZE] = {{-1.835095286369323730e+00, 1.018124699592590332e+00, -2.739300727844238281e+00, 1.015499114990234375e+00},
                                       {1.141516804695129395e+00, 1.435416340827941895e+00, -
                                           2.636226415634155273e+00, -1.633359909057617188e+00},
                                       {1.954798698425292969e-01, -1.445349931716918945e+00,
                                           2.797339484095573425e-02, 1.914618253707885742e+00},
                                       {1.666554093360900879e+00, -1.769495248794555664e+00, -
                                           1.683075547218322754e+00, 1.509697437286376953e+00},
                                       {-1.513341367244720459e-01, 6.947223544120788574e-01, -
                                           1.396772027015686035e+00, -1.296068429946899414e+00},
                                       {-1.771293133497238159e-01, 1.329763531684875488e+00, -
                                           4.773779585957527161e-02, 4.950499534606933594e-01},
                                       {1.049353599548339844e+00, -6.068677306175231934e-01,
                                           7.937226295471191406e-01, -6.256247162818908691e-01},
                                       {-2.388756752014160156e+00, -6.652214527130126953e-01, 1.385061383247375488e+00, 1.787158250808715820e-01}}
float b3[OUTPUT_SIZE] = {-8.084963560104370117e-01,
                         -2.705500088632106781e-02,
                         7.930803894996643066e-01,
                         6.953261047601699829e-02}

#     // Placeholder variable for temporarily storage of matrix operation results
#     float sum;

#     // Layer 1 logic:
#     std::cout << std::endl;
#     std::cout << "Layer 1:" << std::endl;

#     float result1[LAYER_1_SIZE];

#     // Matrix multiplication: input * weight 1 = result1
#     // Matrix addition: result1 + bias 1 = result1

#     for (int i = 0; i < LAYER_1_SIZE; i++)
#     {
#         sum = 0.0;
#         for (int j = 0; j < INPUT_SIZE; j++)
#         {
#             sum += input[j] * w1[j][i];
#         }
#         result1[i] = sum + b1[i];

#         // Apply relu activation function
#         if (result1[i] < 0.0)
#         {
#             result1[i] = 0.0;
#         }
#         std::cout << result1[i] << ", ";
#     }
#     std::cout << std::endl;

#     // Layer 2 logic:
#     std::cout << std::endl;
#     std::cout << "Layer 2:" << std::endl;

#     float result2[LAYER_2_SIZE];

#     // Matrix multiplication: result1 * weight 2 = result2
#     // Matrix addition: result2 + bias 2 = result2

#     for (int i = 0; i < LAYER_2_SIZE; i++)
#     {
#         sum = 0.0;
#         for (int j = 0; j < LAYER_1_SIZE; j++)
#         {
#             sum += result1[j] * w2[j][i];
#         }
#         result2[i] = sum + b2[i];

#         // Apply relu activation function
#         if (result2[i] < 0.0)
#         {
#             result2[i] = 0.0;
#         }
#         std::cout << result2[i];
#         std::cout << ", ";
#     }
#     std::cout << std::endl;

#     // Layer 3 logic:
#     std::cout << std::endl;
#     std::cout << "Layer 3:" << std::endl;

#     float result3[OUTPUT_SIZE];

#     // Matrix multiplication: result2 * weight 3 = result3
#     // Matrix addition: result3 + bias 3 = result3

#     for (int i = 0; i < OUTPUT_SIZE; i++)
#     {
#         sum = 0.0;
#         for (int j = 0; j < LAYER_2_SIZE; j++)
#         {
#             sum += result2[j] * w3[j][i];
#         }
#         result3[i] = sum + b3[i];

#         // Apply softmax activation function
#         sum = 0.0;
#         for (int i = 0; i < OUTPUT_SIZE; i++)
#         {
#             sum += exp(result3[i]);
#         }
#         for (int i = 0; i < OUTPUT_SIZE; i++)
#         {
#             result3[i] = exp(result3[i]) / sum;
#         }
#         std::cout << result3[i] << ", ";
#     }
#     std::cout << std::endl;

#     // Output the classification result
#     int max = 0;
#     for (int i = 0; i < OUTPUT_SIZE; i++)
#     {
#         if (result3[i] > result3[max])
#         {
#             max = i;
#         }
#     }
#     std::cout << std::endl
#               << "Expected result: " << expected_result << std::endl;
#     std::cout << "Actual result: " << max << std::endl;
# }
